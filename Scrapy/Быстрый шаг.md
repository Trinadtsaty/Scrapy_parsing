## Что такое Scrapy?

Scrapy - это мощный фреймворк для парсинга веб-страниц и извлечения данных. Он асинхронный, быстрый и предоставляет все необходимые инструменты для создания парсеров.

## Установка Scrapy

```cmd
# Установка через pip
pip install scrapy

# Или с использованием virtualenv
python -m venv scrapy_env
source scrapy_env/bin/activate  # Linux/Mac
# или
scrapy_env\Scripts\activate     # Windows
pip install scrapy
```

## Создание простого парсера: пошаговая инструкция

### Шаг 1: Создание проекта

``` cmd
scrapy startproject myparser
cd myparser
```

### Шаг 2: Создание паука (spider)

```cmd
scrapy genspider example_spider books.toscrape.com
```
### Шаг 3: Редактирование паука

Откройте файл `myparser/spiders/example_spider.py`:

```python
import scrapy

class ExampleSpiderSpider(scrapy.Spider):
    name = 'example_spider'
    allowed_domains = ['books.toscrape.com']
    start_urls = ['http://books.toscrape.com/']

    def parse(self, response):
        # Извлекаем все книги на странице
        books = response.css('article.product_pod')
        
        for book in books:
            yield {
                'title': book.css('h3 a::attr(title)').get(),
                'price': book.css('p.price_color::text').get(),
                'rating': book.css('p.star-rating::attr(class)').get().split()[-1]
            }
        
        # Переход на следующую страницу
        next_page = response.css('li.next a::attr(href)').get()
        if next_page:
            yield response.follow(next_page, callback=self.parse)
```
### Шаг 4: Запуск парсера

```python
# Сохранение результатов в JSON
scrapy crawl example_spider -o books.json

# Или в CSV
scrapy crawl example_spider -o books.csv

# Для вывода в консоль
scrapy crawl example_spider
```

### Шаг 5: Настройка (опционально)

```python
# Задержка между запросами
DOWNLOAD_DELAY = 1

# User-Agent
USER_AGENT = 'my-parser (+http://www.yourdomain.com)'

# Ограничение глубины
DEPTH_LIMIT = 2
```
## Альтернативный пример: парсер с использованием XPath

```python
import scrapy

class BookSpider(scrapy.Spider):
    name = 'book_spider'
    start_urls = ['http://books.toscrape.com/']

    def parse(self, response):
        # Использование XPath вместо CSS
        for book in response.xpath('//article[@class="product_pod"]'):
            yield {
                'title': book.xpath('.//h3/a/@title').get(),
                'price': book.xpath('.//p[@class="price_color"]/text()').get(),
                'availability': book.xpath('.//p[@class="instock availability"]/text()').get().strip()
            }
        
        # Пагинация
        next_page = response.xpath('//li[@class="next"]/a/@href').get()
        if next_page:
            yield response.follow(next_page, self.parse)
```

## Полезные команды Scrapy

```python
# Просмотр структуры сайта
scrapy shell 'http://books.toscrape.com'

# Запуск с настройками
scrapy crawl example_spider -s FEED_EXPORT_ENCODING='utf-8'

# Ограничение количества страниц
scrapy crawl example_spider -s CLOSESPIDER_PAGECOUNT=10
```
## Преимущества Scrapy:

- **Высокая производительность** (асинхронная обработка)
    
- **Встроенные middleware** для обработки cookies, headers и т.д.
    
- **Поддержка экспорта** в JSON, CSV, XML
    
- **Расширяемость** через pipelines и middleware
    
- **Встроенный селектор** для CSS и XPath
    

Это базовый обзор Scrapy. Фреймворк предлагает множество дополнительных возможностей: обработка форм, авторизация, кастомные middleware, и многое другое.